{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1779d667-58c1-4758-9593-e7372d7094dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cifar100\"\n",
    "iterations = \"2,2,2,2\"\n",
    "u_channels = \"256,256,256,256\"\n",
    "f_channels = \"256,256,256,256\"\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "epoch_step = 30\n",
    "lr = .1\n",
    "lr_step = 10\n",
    "momentum = .9\n",
    "wd = .0005\n",
    "graph = True\n",
    "\n",
    "iterations = [int(x) for x in iterations.split(\",\")]\n",
    "u_channels = [int(x) for x in u_channels.split(\",\")]\n",
    "f_channels = [int(x) for x in f_channels.split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110092fb-21d8-43ad-9647-23c553c84e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae112eb-50d0-4b98-90d1-2f768d69307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    dataset,\n",
    "    split = [\"train\", \"test\"],\n",
    "    as_supervised = True,\n",
    "    with_info = True)\n",
    "\n",
    "rescale = tf.keras.layers.Rescaling(1./255)\n",
    "if dataset == \"mnist\":\n",
    "  mean, variance = [.1307], np.square([.3081])\n",
    "if dataset == \"cifar10\":\n",
    "  mean, variance = [.4914, .4822, .4465], np.square([.2023, .1994, .2010])\n",
    "if dataset == \"cifar100\":\n",
    "  mean, variance = [.5071, .4865, .4409], np.square([.2673, .2564, .2762])\n",
    "normalize = tf.keras.layers.Normalization(mean = mean,\n",
    "                                          variance = variance)\n",
    "\n",
    "def preprocess(ds, training):\n",
    "  if training:\n",
    "    layers = tf.keras.Sequential([\n",
    "      rescale,\n",
    "      tf.keras.layers.RandomTranslation(height_factor = .125,\n",
    "                                        width_factor = .125,\n",
    "                                        fill_mode = \"constant\"),\n",
    "      tf.keras.layers.RandomFlip(mode = \"horizontal\"),\n",
    "      normalize\n",
    "    ])\n",
    "    ds = ds.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "  else:\n",
    "    layers = tf.keras.Sequential([rescale, normalize])\n",
    "\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.map(lambda x, y: (layers(x), y),\n",
    "              num_parallel_calls = tf.data.AUTOTUNE)\n",
    "  ds = ds.cache()\n",
    "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  return ds\n",
    "\n",
    "ds_train = preprocess(ds_train, training = True)\n",
    "ds_test = preprocess(ds_test, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00340774-1918-40b6-bdc7-4a5d22b1c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MgSmooth(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, iterations, u_channels, f_channels):\n",
    "    super(MgSmooth, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.A = tf.keras.layers.Conv2D(u_channels,\n",
    "                                    (3, 3),\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = \"same\",\n",
    "                                    use_bias = False,\n",
    "                                    kernel_initializer = \"he_uniform\")\n",
    "    self.B = tf.keras.layers.Conv2D(f_channels,\n",
    "                                    (3, 3),\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = \"same\",\n",
    "                                    use_bias = False,\n",
    "                                    kernel_initializer = \"he_uniform\")\n",
    "\n",
    "    self.A_bns, self.B_bns = [], []\n",
    "    for _ in range(self.iterations):\n",
    "      self.A_bns.append(tf.keras.layers.BatchNormalization())\n",
    "      self.B_bns.append(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  def call(self, u, f):\n",
    "    for i in range(self.iterations):\n",
    "      error = tf.nn.relu(self.A_bns[i](f - self.A(u)))\n",
    "      u = u + tf.nn.relu(self.B_bns[i](self.B(error)))\n",
    "    return u, f\n",
    "\n",
    "class MgBlock(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, iterations, u_channels, f_channels, A_old):\n",
    "    super(MgBlock, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.Pi = tf.keras.layers.Conv2D(u_channels,\n",
    "                                     (3, 3),\n",
    "                                     strides = (2, 2),\n",
    "                                     padding = \"same\",\n",
    "                                     use_bias = False,\n",
    "                                     kernel_initializer = \"he_uniform\")\n",
    "    self.R = tf.keras.layers.Conv2D(f_channels,\n",
    "                                    (3, 3),\n",
    "                                    strides = (2, 2),\n",
    "                                    padding = \"same\",\n",
    "                                    use_bias = False,\n",
    "                                    kernel_initializer = \"he_uniform\")\n",
    "    self.A_old = A_old\n",
    "    self.MgSmooth = MgSmooth(self.iterations, u_channels, f_channels)\n",
    "\n",
    "    self.Pi_bn = tf.keras.layers.BatchNormalization()\n",
    "    self.R_bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "  def call(self, u0, f0):\n",
    "    u1 = tf.nn.relu(self.Pi_bn(self.Pi(u0)))\n",
    "    error = tf.nn.relu(self.R_bn(self.R(f0 - self.A_old(u0))))\n",
    "    f1 = error + self.MgSmooth.A(u1)\n",
    "    u, f = self.MgSmooth(u1, f1)\n",
    "    return u, f\n",
    "\n",
    "class MgNet(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, iterations, u_channels, f_channels, in_shape, out_shape):\n",
    "    super(MgNet, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.in_shape = in_shape\n",
    "    self.A_init = tf.keras.layers.Conv2D(u_channels[0],\n",
    "                                         (3, 3),\n",
    "                                         strides = (1, 1),\n",
    "                                         padding = \"same\",\n",
    "                                         use_bias = False,\n",
    "                                         kernel_initializer = \"he_uniform\")\n",
    "    self.A_bn = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.blocks = []\n",
    "    for i in range(len(self.iterations)):\n",
    "      if i == 0:\n",
    "        self.blocks.append(MgSmooth(iterations[i],\n",
    "                                    u_channels[i],\n",
    "                                    f_channels[i]))\n",
    "        continue\n",
    "      if i == 1:\n",
    "        self.blocks.append(MgBlock(iterations[i],\n",
    "                                   u_channels[i],\n",
    "                                   f_channels[i],\n",
    "                                   self.blocks[0].A))\n",
    "        continue\n",
    "      self.blocks.append(MgBlock(iterations[i],\n",
    "                                 u_channels[i],\n",
    "                                 f_channels[i],\n",
    "                                 self.blocks[i - 1].MgSmooth.A))\n",
    "\n",
    "    x = in_shape[0]\n",
    "    for i in range(len(self.blocks) - 1):\n",
    "      x = ((x + 2 - 3) // 2) + 1\n",
    "    self.pool = tf.keras.layers.AveragePooling2D(pool_size = (x, x))\n",
    "    self.fc = tf.keras.layers.Dense(out_shape,\n",
    "                                    activation = \"softmax\")\n",
    "  \n",
    "  def call(self, u0):\n",
    "    f = tf.nn.relu(self.A_bn(self.A_init(u0)))\n",
    "    u = tf.multiply(f, 0)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      u, f = block(u, f)\n",
    "    u = self.pool(u)\n",
    "    u = tf.squeeze(u, [-2, -3])\n",
    "    u = self.fc(u)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e45c82-9eae-42dd-9dab-f291eb6047fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "  if (epoch + 1) % epoch_step == 0:\n",
    "    return lr / lr_step\n",
    "  return lr\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "with strategy.scope():\n",
    "  model = MgNet(iterations = iterations,\n",
    "                u_channels = u_channels,\n",
    "                f_channels = f_channels,\n",
    "                in_shape = ds_info.features[\"image\"].shape,\n",
    "                out_shape = ds_info.features[\"label\"].num_classes)\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "  lr_s = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "  optimizer = tfa.optimizers.SGDW(learning_rate = lr,\n",
    "                                  weight_decay = wd,\n",
    "                                  momentum = momentum)\n",
    "  \n",
    "  model.compile(optimizer = optimizer,\n",
    "                loss = loss,\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "  history = model.fit(ds_train,\n",
    "                      epochs = epochs,\n",
    "                      validation_data = ds_test,\n",
    "                      callbacks = [lr_s])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609c21a-9301-487a-ab14-4bb270321d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if graph:\n",
    "  loss = history.history[\"loss\"]\n",
    "  accuracy = history.history[\"accuracy\"]\n",
    "  val_loss = history.history[\"val_loss\"]\n",
    "  val_accuracy = history.history[\"val_accuracy\"]\n",
    "  timerange = range(len(loss))\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  train_loss_plot, = ax.plot(timerange, loss, color = \"blue\")\n",
    "  val_loss_plot, = ax.plot(timerange, val_loss, color = \"cyan\")\n",
    "  train_loss_plot.set_label(\"Train Loss\")\n",
    "  val_loss_plot.set_label(\"Validation Loss\")\n",
    "  ax.set_xlabel(\"Epoch\")\n",
    "  ax.set_ylabel(\"Loss\")\n",
    "  ax.legend(loc = \"upper left\")\n",
    "  ax2 = ax.twinx()\n",
    "  train_acc_plot, = ax2.plot(timerange, accuracy, color = \"purple\")\n",
    "  val_acc_plot, = ax2.plot(timerange, val_accuracy, color = \"pink\")\n",
    "  train_acc_plot.set_label(\"Train Accuracy\")\n",
    "  val_acc_plot.set_label(\"Validation Accuracy\")\n",
    "  ax2.set_ylabel(\"Accuracy\")\n",
    "  ax2.legend(loc = \"upper right\")\n",
    "  plt.title(\"Loss vs Accuracy\")\n",
    "  plt.savefig(f\"{dataset}_mgnet_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74dbcf-d1c4-4c82-b1ac-391a842e4284",
   "metadata": {},
   "source": [
    "# **PyTorch Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e33b2e7-f7e3-4308-8f96-81e69ffc8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d2a8d3-f829-4971-b3d2-abe77451547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MgIte(nn.Module):\n",
    "  def __init__(self, A, B):\n",
    "    super().__init__()\n",
    "    self.A = A\n",
    "    self.B = B        \n",
    "    self.bn1 =nn.BatchNorm2d(A.weight.size(0))\n",
    "    self.bn2 =nn.BatchNorm2d(B.weight.size(0))\n",
    "\n",
    "  def forward(self, out):\n",
    "    u, f = out\n",
    "    u = u + F.relu(self.bn2(self.B(F.relu(self.bn1((f-self.A(u)))))))\n",
    "    out = (u, f)\n",
    "    return out\n",
    "    \n",
    "class MgRestriction(nn.Module):\n",
    "  def __init__(self, A_old, A_conv, Pi_conv, R_conv):\n",
    "    super().__init__()\n",
    "    self.A_old = A_old\n",
    "    self.A_conv = A_conv\n",
    "    self.Pi_conv = Pi_conv\n",
    "    self.R_conv = R_conv\n",
    "\n",
    "    self.bn1 = nn.BatchNorm2d(Pi_conv.weight.size(0))\n",
    "    self.bn2 = nn.BatchNorm2d(A_old.weight.size(0))\n",
    "\n",
    "  def forward(self, out):\n",
    "    u_old, f_old = out\n",
    "    u = F.relu(self.bn1(self.Pi_conv(u_old)))\n",
    "    f = F.relu(self.bn2(self.R_conv(f_old-self.A_old(u_old)))) + self.A_conv(u)\n",
    "    out = (u,f)\n",
    "    return out\n",
    "\n",
    "class MgNet(nn.Module):\n",
    "  def __init__(self, args,num_classes):\n",
    "    super().__init__()\n",
    "    self.num_iteration = args.num_ite\n",
    "    self.args = args\n",
    "    \n",
    "    # inilization layer\n",
    "    if args.dataset == 'mnist':\n",
    "      self.num_channel_input=1\n",
    "    else:\n",
    "      self.num_channel_input=3\n",
    "    self.conv1 = nn.Conv2d(self.num_channel_input, args.num_channel_f, kernel_size=3, stride=1,\\\n",
    "                            padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(args.num_channel_f)        \n",
    "\n",
    "    A_conv = nn.Conv2d(args.num_channel_u, args.num_channel_f, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    if not args.wise_B:\n",
    "      B_conv = nn.Conv2d(args.num_channel_f, args.num_channel_u, kernel_size=3,stride=1, padding=1, bias=False)\n",
    "\n",
    "    layers = []\n",
    "    for l, num_iteration_l in enumerate(self.num_iteration):\n",
    "      for i in range(num_iteration_l):\n",
    "        if args.wise_B:\n",
    "          B_conv = nn.Conv2d(args.num_channel_f, args.num_channel_u, kernel_size=3,\\\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        \n",
    "        layers.append(MgIte(A_conv, B_conv))\n",
    "\n",
    "      setattr(self, 'layer'+str(l), nn.Sequential(*layers))\n",
    "\n",
    "\n",
    "      if l < len(self.num_iteration)-1:\n",
    "        A_old = A_conv\n",
    "        #B_old = B_conv\n",
    "        A_conv = nn.Conv2d(args.num_channel_u, args.num_channel_f, kernel_size=3,\\\n",
    "                            stride=1, padding=1, bias=False)\n",
    "        if not args.wise_B:\n",
    "          B_conv = nn.Conv2d(args.num_channel_f, args.num_channel_u, kernel_size=3,\\\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        Pi_conv = nn.Conv2d(args.num_channel_u, args.num_channel_u, kernel_size=3,\\\n",
    "                            stride=2, padding=1, bias=False)\n",
    "        R_conv = nn.Conv2d(args.num_channel_f, args.num_channel_u, kernel_size=3, \\\n",
    "                            stride=2, padding=1, bias=False)\n",
    "        layers= [MgRestriction(A_old, A_conv, Pi_conv, R_conv)]\n",
    "\n",
    "    self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "    self.fc = nn.Linear(args.num_channel_u ,num_classes)\n",
    "\n",
    "  def forward(self, u):\n",
    "    f = F.relu(self.bn1(self.conv1(u)))\n",
    "    if torch.cuda.is_available():\n",
    "      u = torch.zeros(f.size(),device=torch.device('cuda'))\n",
    "    else:\n",
    "      u = torch.zeros(f.size())\n",
    "    out = (u, f)\n",
    "\n",
    "    for l in range(len(self.num_iteration)):\n",
    "      out = getattr(self, 'layer'+str(l))(out)\n",
    "    u,f = out\n",
    "    u = self.pooling(u)\n",
    "    u = u.view(u.shape[0], -1)\n",
    "    u = self.fc(u)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658e386c-e11f-45be-a8e5-8e0750d91783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr):\n",
    "  #lr = 1.0 / (epoch + 1)\n",
    "  lr = init_lr * 0.1 ** (epoch // 50)\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "  return lr\n",
    "\n",
    "def load_data(path,minibatch_size,dataset):\n",
    "  if dataset=='mnist':\n",
    "    trainloader = torch.utils.data.DataLoader(torchvision.datasets.MNIST(root=path, train=True, download=True,\n",
    "                                      transform=torchvision.transforms.Compose([\n",
    "                                      torchvision.transforms.ToTensor(),\n",
    "                                      torchvision.transforms.Normalize(\n",
    "                                      (0.1307,), (0.3081,))\n",
    "                                      ])),\n",
    "                                      batch_size=minibatch_size, num_workers=4,shuffle=True)\n",
    "                                      \n",
    "    testloader = torch.utils.data.DataLoader(torchvision.datasets.MNIST(root=path, train=False, download=True,\n",
    "                                      transform=torchvision.transforms.Compose([\n",
    "                                      torchvision.transforms.ToTensor(),\n",
    "                                      torchvision.transforms.Normalize(\n",
    "                                      (0.1307,), (0.3081,))\n",
    "                                      ])),\n",
    "                                      batch_size=minibatch_size, num_workers=4,shuffle=True)\n",
    "    num_classes = 10\n",
    "  if dataset=='cifar10':\n",
    "    normalize = torchvision.transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "    transform_train = torchvision.transforms.Compose([torchvision.transforms.RandomCrop(32, padding=4),\\\n",
    "                                    torchvision.transforms.RandomHorizontalFlip(),\\\n",
    "                                    torchvision.transforms.ToTensor(),\\\n",
    "                                    normalize])\n",
    "\n",
    "    transform_test  = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),normalize])\n",
    "\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root=path, train=True,download=True,transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=minibatch_size,num_workers=4, shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root=path, train=False, download=True,transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=minibatch_size,num_workers=4, shuffle=False)\n",
    "    num_classes = 10\n",
    "  if dataset=='cifar100':\n",
    "    normalize = torchvision.transforms.Normalize(mean=(0.5071, 0.4865, 0.4409), std=(0.2673, 0.2564, 0.2762))\n",
    "    transform_train = torchvision.transforms.Compose([torchvision.transforms.RandomCrop(32, padding=4),\\\n",
    "                                    torchvision.transforms.RandomHorizontalFlip(),\\\n",
    "                                    torchvision.transforms.ToTensor(),\\\n",
    "                                    normalize])\n",
    "\n",
    "    transform_test  = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),normalize])\n",
    "\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR100(root=path, train=True,download=True,transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=minibatch_size,num_workers=4, shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR100(root=path, train=False, download=True,transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=minibatch_size,num_workers=4, shuffle=False)\n",
    "    num_classes = 100\n",
    "  \n",
    "  return trainloader,testloader,num_classes\n",
    "\n",
    "def train_process(model,save_name,num_epochs,lr,trainloader,testloader):\n",
    "  test_acc = []\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay = 0.0005)\n",
    "  optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "  print(\"total {} paramerters\".format(sum(x.numel() for x in model.parameters())))\n",
    "\n",
    "  start = timer()\n",
    "  Test_acc = 0\n",
    "  for epoch in range(1,num_epochs+1):\n",
    "    current_lr = adjust_learning_rate(optimizer, epoch, lr)\n",
    "\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "      if use_cuda:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "      # Forward pass to get the loss\n",
    "      outputs = model(images) \n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      # Backward and compute the gradient\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()  #backpropragation\n",
    "      optimizer.step() #update the weights/parameters\n",
    "\n",
    "    # Training accuracy \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "      with torch.no_grad():\n",
    "        if use_cuda:\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "        outputs = model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    training_accuracy = float(correct)/total\n",
    "\n",
    "    # Test accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(testloader):\n",
    "      with torch.no_grad():\n",
    "        if use_cuda:\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "        outputs = model(images)\n",
    "        p_max, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "    test_accuracy = float(correct)/total\n",
    "    test_acc.append(test_accuracy)\n",
    "    if test_accuracy>Test_acc:\n",
    "      # save(model, optimizer,save_name,epoch)\n",
    "      Test_acc = test_accuracy\n",
    "\n",
    "    print('Epoch:{}, loss:{:.2f}, lr:{:.6f}, train acc:{:.4f}, test acc:{:.4f}, best acc:{:.4f}'.\\\n",
    "           format(epoch,loss,current_lr,training_accuracy,test_accuracy,np.max(test_acc)))\n",
    "\n",
    "  #end = timer()\n",
    "  print('last_train_acc:{:.4f},max_acc:{:.4f},last_acc:{:.4f}'.format(training_accuracy,np.max(test_acc),test_accuracy))\n",
    "  return training_accuracy,np.max(test_acc),test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fff2d-2866-4dc8-8690-412af8736a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_obj():\n",
    "  def __init__(self):\n",
    "    self.dataset = \"cifar100\"\n",
    "    self.path = \"~/pytorch_datasets\"\n",
    "    self.num_ite = \"2,2,2,2\"\n",
    "    self.num_channel_u = 256\n",
    "    self.num_channel_f = 256\n",
    "    self.wise_B = False\n",
    "    self.minibatch_size = 128\n",
    "    self.num_epoch = 300\n",
    "    self.lr = .1\n",
    "    \n",
    "args = args_obj()\n",
    "args.num_ite = [int(i) for i in args.num_ite.split(',')]\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "trainloader,testloader,num_classes = load_data(args.path,args.minibatch_size,args.dataset)\n",
    "model = MgNet(args,num_classes=num_classes)\n",
    "if use_cuda:\n",
    "  model =model.cuda()\n",
    "train_acc,test_max_acc,test_last_acc = train_process(model,\"\",args.num_epoch,args.lr,trainloader,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adceb3d7-d03a-47cd-a47f-befaf1275bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
