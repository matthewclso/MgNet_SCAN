{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2fb808-a085-4b66-8df8-6d89bf9bfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - initialization\n",
    "# - batchnorm parameters\n",
    "# - weight decay and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1583f1-b173-407c-af88-5e25f865d21a",
   "metadata": {},
   "source": [
    "# **TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1779d667-58c1-4758-9593-e7372d7094dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cifar100\"\n",
    "iterations = \"2,2,2,2\"\n",
    "u_channels = \"256,256,256,256\"\n",
    "f_channels = \"256,256,256,256\"\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "epoch_step = 30\n",
    "lr = .1\n",
    "lr_step = 10\n",
    "momentum = .9\n",
    "wd = .0005\n",
    "wd = 0\n",
    "\n",
    "iterations = [int(x) for x in iterations.split(\",\")]\n",
    "u_channels = [int(x) for x in u_channels.split(\",\")]\n",
    "f_channels = [int(x) for x in f_channels.split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110092fb-21d8-43ad-9647-23c553c84e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae112eb-50d0-4b98-90d1-2f768d69307a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    dataset,\n",
    "    split = [\"train\", \"test\"],\n",
    "    as_supervised = True,\n",
    "    with_info = True)\n",
    "\n",
    "rescale = tf.keras.layers.Rescaling(1. / 255)\n",
    "if dataset == \"mnist\":\n",
    "  mean, variance = [.1307], np.square([.3081])\n",
    "if dataset == \"cifar10\":\n",
    "  mean, variance = [.4914, .4822, .4465], np.square([.2023, .1994, .2010])\n",
    "if dataset == \"cifar100\":\n",
    "  mean, variance = [.5071, .4865, .4409], np.square([.2673, .2564, .2762])\n",
    "normalize = tf.keras.layers.Normalization(mean = mean,\n",
    "                                          variance = variance)\n",
    "\n",
    "def preprocess(ds, training):\n",
    "  if training:\n",
    "    layers = tf.keras.Sequential([\n",
    "      rescale,\n",
    "      tf.keras.layers.RandomTranslation(height_factor = .125,\n",
    "                                        width_factor = .125,\n",
    "                                        fill_mode = \"constant\"),\n",
    "      tf.keras.layers.RandomFlip(mode = \"horizontal\"),\n",
    "      normalize\n",
    "    ])\n",
    "    ds = ds.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "  else:\n",
    "    layers = tf.keras.Sequential([rescale, normalize])\n",
    "\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.map(lambda x, y: (layers(x), y),\n",
    "              num_parallel_calls = tf.data.AUTOTUNE)\n",
    "  ds = ds.cache()\n",
    "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  return ds\n",
    "\n",
    "ds_train = preprocess(ds_train, training = True)\n",
    "ds_test = preprocess(ds_test, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7bffd85-228b-49f1-8442-30a4027a2b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HeUniform(tf.keras.initializers.Initializer):\n",
    "  \n",
    "  def __init__(self, a, mode, nonlinearity, bound = None):\n",
    "    self.a = a\n",
    "    self.mode = mode\n",
    "    self.nonlinearity = nonlinearity\n",
    "    self.bound = bound\n",
    "    \n",
    "    if self.nonlinearity == \"sigmoid\":\n",
    "      self.gain = 1\n",
    "    elif self.nonlinearity == \"tanh\":\n",
    "      self.gain = 5.0 / 3\n",
    "    elif self.nonlinearity == \"relu\":\n",
    "      self.gain = np.sqrt(2.0)\n",
    "    elif self.nonlinearity == \"leaky_relu\":\n",
    "      if self.a is None:\n",
    "        self.gain = .01\n",
    "      else:\n",
    "        self.gain = np.sqrt(2.0 / (1 + self.a ** 2))\n",
    "    elif self.nonlinearity == \"selu\":\n",
    "      self.gain = 3.0 / 4\n",
    "    \n",
    "  def __call__(self, shape, dtype = None, **kwargs):\n",
    "    if self.bound:\n",
    "      return tf.random.uniform(shape,\n",
    "                               minval = -self.bound,\n",
    "                               maxval = self.bound)\n",
    "    \n",
    "    torch_shape = np.flip(shape)\n",
    "    \n",
    "    num_input_fmaps = torch_shape[1]\n",
    "    num_output_fmaps = torch_shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if len(torch_shape) > 2:\n",
    "      for s in torch_shape[2:]:\n",
    "        receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size    \n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "    \n",
    "    if self.mode == \"fan_in\":\n",
    "      fan = fan_in\n",
    "    elif self.mode == \"fan_out\":\n",
    "      fan = fan_out\n",
    "      \n",
    "    std = self.gain / np.sqrt(fan)\n",
    "    bound = np.sqrt(3.0) * std\n",
    "    \n",
    "    return tf.random.uniform(shape,\n",
    "                             minval = -bound,\n",
    "                             maxval = bound)\n",
    "  \n",
    "class Conv2D(tf.keras.layers.Layer):\n",
    "  \n",
    "  def __init__(self,\n",
    "               filters,\n",
    "               kernel_size,\n",
    "               strides = (1, 1),\n",
    "               padding = \"valid\",\n",
    "               data_format = None,\n",
    "               dilation_rate = (1, 1),\n",
    "               groups = 1,\n",
    "               activation = None,\n",
    "               use_bias = True,\n",
    "               kernel_initializer = None,\n",
    "               bias_initializer = \"zeros\",\n",
    "               kernel_regularizer = None,\n",
    "               bias_regularizer = None,\n",
    "               activity_regularizer = None,\n",
    "               kernel_constraint = None,\n",
    "               bias_constraint = None,\n",
    "               **kwargs\n",
    "              ):\n",
    "    super(Conv2D, self).__init__()\n",
    "    \n",
    "    if kernel_initializer is None:\n",
    "      kernel_initializer = HeUniform(np.sqrt(5),\n",
    "                                     \"fan_in\",\n",
    "                                     \"leaky_relu\")\n",
    "      \n",
    "    self.torch_padding = None\n",
    "    if isinstance(padding, list) or isinstance(padding, tuple):\n",
    "      self.torch_padding = padding\n",
    "      padding = \"valid\"\n",
    "      \n",
    "    self.conv2d = tf.keras.layers.Conv2D(filters = filters,\n",
    "                                         kernel_size = kernel_size,\n",
    "                                         strides = strides,\n",
    "                                         padding = padding,\n",
    "                                         data_format = data_format,\n",
    "                                         dilation_rate = dilation_rate,\n",
    "                                         groups = groups,\n",
    "                                         activation = activation,\n",
    "                                         use_bias = use_bias,\n",
    "                                         kernel_initializer = kernel_initializer,\n",
    "                                         bias_initializer = bias_initializer,\n",
    "                                         kernel_regularizer = kernel_regularizer,\n",
    "                                         bias_regularizer = bias_regularizer,\n",
    "                                         activity_regularizer = activity_regularizer,\n",
    "                                         kernel_constraint = kernel_constraint,\n",
    "                                         bias_constraint = bias_constraint,\n",
    "                                         **kwargs\n",
    "                                        )\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    if self.torch_padding:\n",
    "      inputs = tf.pad(inputs,\n",
    "                      [[0, 0],\n",
    "                      [self.torch_padding[0], self.torch_padding[0]],\n",
    "                      [self.torch_padding[1], self.torch_padding[1]],\n",
    "                      [0, 0]],\n",
    "                      \"CONSTANT\")\n",
    "    out = self.conv2d(inputs)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00340774-1918-40b6-bdc7-4a5d22b1c56e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MgSmooth(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self,\n",
    "               iterations,\n",
    "               u_channels,\n",
    "               f_channels,\n",
    "               wd):\n",
    "    super(MgSmooth, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.A = Conv2D(u_channels,\n",
    "                    (3, 3),\n",
    "                    strides = (1, 1),\n",
    "                    padding = (1, 1),\n",
    "                    use_bias = False,\n",
    "                    kernel_regularizer = \n",
    "                      tf.keras.regularizers.L2(wd))\n",
    "    self.B = Conv2D(u_channels,\n",
    "                    (3, 3),\n",
    "                    strides = (1, 1),\n",
    "                    padding = (1, 1),\n",
    "                    use_bias = False,\n",
    "                    kernel_regularizer = \n",
    "                      tf.keras.regularizers.L2(wd))\n",
    "\n",
    "    self.A_bns, self.B_bns = [], []\n",
    "    for _ in range(self.iterations):\n",
    "      self.A_bns.append(tf.keras.layers.BatchNormalization(momentum = .9,\n",
    "                                                           epsilon = 1e-5))\n",
    "      self.B_bns.append(tf.keras.layers.BatchNormalization(momentum = .9,\n",
    "                                                           epsilon = 1e-5))\n",
    "\n",
    "  def call(self, u, f):\n",
    "    for i in range(self.iterations):\n",
    "      error = tf.nn.relu(self.A_bns[i](f - self.A(u)))\n",
    "      u = u + tf.nn.relu(self.B_bns[i](self.B(error)))\n",
    "    return u, f\n",
    "\n",
    "class MgBlock(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self,\n",
    "               iterations,\n",
    "               u_channels,\n",
    "               f_channels,\n",
    "               A_old,\n",
    "               wd):\n",
    "    super(MgBlock, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.Pi = Conv2D(u_channels,\n",
    "                     (3, 3),\n",
    "                     strides = (2, 2),\n",
    "                     padding = (1, 1),\n",
    "                     use_bias = False,\n",
    "                     kernel_regularizer = \n",
    "                       tf.keras.regularizers.L2(wd))\n",
    "    self.R = Conv2D(u_channels,\n",
    "                    (3, 3),\n",
    "                    strides = (2, 2),\n",
    "                    padding = (1, 1),\n",
    "                    use_bias = False,\n",
    "                    kernel_regularizer = \n",
    "                      tf.keras.regularizers.L2(wd))\n",
    "    self.A_old = A_old\n",
    "    self.MgSmooth = MgSmooth(self.iterations,\n",
    "                             u_channels,\n",
    "                             f_channels,\n",
    "                             wd)\n",
    "\n",
    "    self.Pi_bn = tf.keras.layers.BatchNormalization(momentum = .9,\n",
    "                                                    epsilon = 1e-5)\n",
    "    self.R_bn = tf.keras.layers.BatchNormalization(momentum = .9,\n",
    "                                                   epsilon = 1e-5)\n",
    "\n",
    "  def call(self, u0, f0):\n",
    "    u1 = tf.nn.relu(self.Pi_bn(self.Pi(u0)))\n",
    "    error = tf.nn.relu(self.R_bn(self.R(f0 - self.A_old(u0))))\n",
    "    f1 = error + self.MgSmooth.A(u1)\n",
    "    u, f = self.MgSmooth(u1, f1)\n",
    "    return u, f\n",
    "\n",
    "class MgNet(tf.keras.Model):\n",
    "\n",
    "  def __init__(self,\n",
    "               iterations,\n",
    "               u_channels,\n",
    "               f_channels,\n",
    "               in_shape,\n",
    "               out_shape,\n",
    "               wd):\n",
    "    super(MgNet, self).__init__()\n",
    "\n",
    "    self._name = \"mgnet_tensorflow\"\n",
    "    self.iterations = iterations\n",
    "    self.in_shape = in_shape\n",
    "    self.A_init = Conv2D(u_channels[0],\n",
    "                         (3, 3),\n",
    "                         strides = (1, 1),\n",
    "                         padding = (1, 1),\n",
    "                         use_bias = False,\n",
    "                         kernel_regularizer = \n",
    "                           tf.keras.regularizers.L2(wd))\n",
    "    self.A_bn = tf.keras.layers.BatchNormalization(momentum = .9,\n",
    "                                                   epsilon = 1e-5)\n",
    "\n",
    "    self.blocks = []\n",
    "    for i in range(len(self.iterations)):\n",
    "      if i == 0:\n",
    "        self.blocks.append(MgSmooth(iterations[i],\n",
    "                                    u_channels[i],\n",
    "                                    f_channels[i],\n",
    "                                    wd))\n",
    "        continue\n",
    "      if i == 1:\n",
    "        self.blocks.append(MgBlock(iterations[i],\n",
    "                                   u_channels[i],\n",
    "                                   f_channels[i],\n",
    "                                   self.blocks[0].A,\n",
    "                                   wd))\n",
    "        continue\n",
    "      self.blocks.append(MgBlock(iterations[i],\n",
    "                                 u_channels[i],\n",
    "                                 f_channels[i],\n",
    "                                 self.blocks[i - 1].MgSmooth.A,\n",
    "                                 wd))\n",
    "\n",
    "    x = in_shape[0]\n",
    "    for i in range(len(self.blocks) - 1):\n",
    "      x = ((x + 2 - 3) // 2) + 1\n",
    "    self.pool = tf.keras.layers.AveragePooling2D(pool_size = (x, x))\n",
    "    self.fc = tf.keras.layers.Dense(out_shape,\n",
    "                                    kernel_initializer = \n",
    "                                      HeUniform(np.sqrt(5),\n",
    "                                                \"fan_in\",\n",
    "                                                \"leaky_relu\"),\n",
    "                                    bias_initializer = \n",
    "                                      HeUniform(np.sqrt(5),\n",
    "                                                \"fan_in\",\n",
    "                                                \"leaky_relu\",\n",
    "                                                1 / np.sqrt(u_channels[-1])),\n",
    "                                    kernel_regularizer = \n",
    "                                      tf.keras.regularizers.L2(wd))\n",
    "    \n",
    "    self.A_init._name = \"initial_A_conv\"\n",
    "    self.A_bn._name = \"initial_A_bn\"\n",
    "    for i, block in enumerate(self.blocks):\n",
    "      block._name = f\"block{i}\"\n",
    "      if i == 0:\n",
    "        block.A._name = \"block0_A_conv\"\n",
    "        block.B._name = \"block0_B_conv\"\n",
    "        for j, bn in enumerate(block.A_bns):\n",
    "          bn._name = f\"block0_A_batchnorm{j}\"\n",
    "        for j, bn in enumerate(block.B_bns):\n",
    "          bn._name = f\"block0_B_batchnorm{j}\"\n",
    "      else:\n",
    "        block.MgSmooth._name = f\"block{i}_MgSmooth\"\n",
    "        block.MgSmooth.A._name = f\"block{i}_A_conv\"\n",
    "        block.MgSmooth.B._name = f\"block{i}_B_conv\"\n",
    "        for j, bn in enumerate(block.MgSmooth.A_bns):\n",
    "          bn._name = f\"block{i}_A_batchnorm{j}\"\n",
    "        for j, bn in enumerate(block.MgSmooth.B_bns):\n",
    "          bn._name = f\"block{i}_B_batchnorm{j}\"\n",
    "        block.Pi._name = f\"block{i}_Pi_conv\"\n",
    "        block.R._name = f\"block{i}_R_conv\"\n",
    "        block.Pi_bn._name = f\"block{i}_Pi_batchnorm\"\n",
    "        block.R_bn._name = f\"block{i}_R_batchnorm\"\n",
    "    self.pool._name = \"final_average_pool\"\n",
    "    self.fc._name = \"output_softmax\"\n",
    "  \n",
    "  def call(self, u0):\n",
    "    f = tf.nn.relu(self.A_bn(self.A_init(u0)))\n",
    "    u = tf.multiply(f, 0)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      u, f = block(u, f)\n",
    "    u = self.pool(u)\n",
    "    u = tf.squeeze(u, [-2, -3])\n",
    "    u = self.fc(u)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5fe004e-ce1e-4e9f-be21-837aa485460d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# no batchnorm\n",
    "\n",
    "class MgSmooth(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self,\n",
    "               iterations,\n",
    "               u_channels,\n",
    "               f_channels,\n",
    "               wd):\n",
    "    super(MgSmooth, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.A = Conv2D(u_channels,\n",
    "                    (3, 3),\n",
    "                    strides = (1, 1),\n",
    "                    padding = (1, 1),\n",
    "                    use_bias = False,\n",
    "                    kernel_regularizer = \n",
    "                      tf.keras.regularizers.L2(wd))\n",
    "    self.B = Conv2D(f_channels,\n",
    "                    (3, 3),\n",
    "                    strides = (1, 1),\n",
    "                    padding = (1, 1),\n",
    "                    use_bias = False,\n",
    "                    kernel_regularizer = \n",
    "                      tf.keras.regularizers.L2(wd))\n",
    "\n",
    "  def call(self, u, f):\n",
    "    for i in range(self.iterations):\n",
    "      error = tf.nn.relu((f - self.A(u)))\n",
    "      u = u + tf.nn.relu((self.B(error)))\n",
    "    return u, f\n",
    "\n",
    "class MgBlock(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self,\n",
    "               iterations,\n",
    "               u_channels,\n",
    "               f_channels,\n",
    "               A_old,\n",
    "               wd):\n",
    "    super(MgBlock, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.Pi = Conv2D(u_channels,\n",
    "                     (3, 3),\n",
    "                     strides = (2, 2),\n",
    "                     padding = (1, 1),\n",
    "                     use_bias = False,\n",
    "                     kernel_regularizer = \n",
    "                       tf.keras.regularizers.L2(wd))\n",
    "    self.R = Conv2D(f_channels,\n",
    "                    (3, 3),\n",
    "                    strides = (2, 2),\n",
    "                    padding = (1, 1),\n",
    "                    use_bias = False,\n",
    "                    kernel_regularizer = \n",
    "                      tf.keras.regularizers.L2(wd))\n",
    "    self.A_old = A_old\n",
    "    self.MgSmooth = MgSmooth(self.iterations,\n",
    "                             u_channels,\n",
    "                             f_channels,\n",
    "                             wd)\n",
    "\n",
    "  def call(self, u0, f0):\n",
    "    u1 = tf.nn.relu((self.Pi(u0)))\n",
    "    error = tf.nn.relu((self.R(f0 - self.A_old(u0))))\n",
    "    f1 = error + self.MgSmooth.A(u1)\n",
    "    u, f = self.MgSmooth(u1, f1)\n",
    "    return u, f\n",
    "\n",
    "class MgNet(tf.keras.Model):\n",
    "\n",
    "  def __init__(self,\n",
    "               iterations,\n",
    "               u_channels,\n",
    "               f_channels,\n",
    "               in_shape,\n",
    "               out_shape,\n",
    "               wd):\n",
    "    super(MgNet, self).__init__()\n",
    "\n",
    "    self._name = \"mgnet_tensorflow\"\n",
    "    self.iterations = iterations\n",
    "    self.in_shape = in_shape\n",
    "    self.A_init = Conv2D(u_channels[0],\n",
    "                         (3, 3),\n",
    "                         strides = (1, 1),\n",
    "                         padding = (1, 1),\n",
    "                         use_bias = False,\n",
    "                         kernel_regularizer = \n",
    "                           tf.keras.regularizers.L2(wd))\n",
    "\n",
    "    self.blocks = []\n",
    "    for i in range(len(self.iterations)):\n",
    "      if i == 0:\n",
    "        self.blocks.append(MgSmooth(iterations[i],\n",
    "                                    u_channels[i],\n",
    "                                    f_channels[i],\n",
    "                                    wd))\n",
    "        continue\n",
    "      if i == 1:\n",
    "        self.blocks.append(MgBlock(iterations[i],\n",
    "                                   u_channels[i],\n",
    "                                   f_channels[i],\n",
    "                                   self.blocks[0].A,\n",
    "                                   wd))\n",
    "        continue\n",
    "      self.blocks.append(MgBlock(iterations[i],\n",
    "                                 u_channels[i],\n",
    "                                 f_channels[i],\n",
    "                                 self.blocks[i - 1].MgSmooth.A,\n",
    "                                 wd))\n",
    "\n",
    "    x = in_shape[0]\n",
    "    for i in range(len(self.blocks) - 1):\n",
    "      x = ((x + 2 - 3) // 2) + 1\n",
    "    self.pool = tf.keras.layers.AveragePooling2D(pool_size = (x, x))\n",
    "    self.fc = tf.keras.layers.Dense(out_shape,\n",
    "                                    kernel_initializer = \n",
    "                                      tf.keras.initializers.Constant(0.),\n",
    "                                    bias_initializer = \n",
    "                                      tf.keras.initializers.Constant(0.),\n",
    "                                    kernel_regularizer = \n",
    "                                      tf.keras.regularizers.L2(wd))\n",
    "    \n",
    "    self.A_init._name = \"initial_A_conv\"\n",
    "    for i, block in enumerate(self.blocks):\n",
    "      block._name = f\"block{i}\"\n",
    "      if i == 0:\n",
    "        block.A._name = \"block0_A_conv\"\n",
    "        block.B._name = \"block0_B_conv\"\n",
    "      else:\n",
    "        block.MgSmooth._name = f\"block{i}_MgSmooth\"\n",
    "        block.MgSmooth.A._name = f\"block{i}_A_conv\"\n",
    "        block.MgSmooth.B._name = f\"block{i}_B_conv\"\n",
    "        block.Pi._name = f\"block{i}_Pi_conv\"\n",
    "        block.R._name = f\"block{i}_R_conv\"\n",
    "    self.pool._name = \"final_average_pool\"\n",
    "    self.fc._name = \"output_softmax\"\n",
    "  \n",
    "  def call(self, u0):\n",
    "    f = tf.nn.relu((self.A_init(u0)))\n",
    "    u = tf.multiply(f, 0)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      u, f = block(u, f)\n",
    "      # f_temp = tf.transpose(f, [0, 3, 1, 2])\n",
    "      # print(f_temp[0][0][0])\n",
    "    u = self.pool(u)\n",
    "    u = tf.squeeze(u, [-2, -3])\n",
    "    u = self.fc(u)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054f9d93-5982-410e-b0b0-ba4136fe8b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_weights(writer, model, epoch, grads):\n",
    "  with writer.as_default():\n",
    "    with tf.summary.record_if(True):\n",
    "      for layer in model.layers:\n",
    "        for weight in layer.weights:\n",
    "          weight_name = weight.name.replace(\":\", \"_\")\n",
    "          histogram_weight_name = f\"{model.name}{weight_name}\"\n",
    "          tf.summary.histogram(histogram_weight_name,\n",
    "                               weight,\n",
    "                               step = epoch)\n",
    "      if grads:\n",
    "        weight_names = [x.name.replace(\":\", \"_\") for x in model.trainable_weights]\n",
    "        for i, grad in enumerate(grads):\n",
    "          tf.summary.histogram(weight_names[i] + \"_gradient\",\n",
    "                               grad,\n",
    "                               step = epoch)\n",
    "      writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e45c82-9eae-42dd-9dab-f291eb6047fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "  if (epoch + 1) % epoch_step == 0:\n",
    "    return lr / lr_step\n",
    "  return lr\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "with strategy.scope():\n",
    "  model = MgNet(iterations = iterations,\n",
    "                u_channels = u_channels,\n",
    "                f_channels = f_channels,\n",
    "                in_shape = ds_info.features[\"image\"].shape,\n",
    "                out_shape = ds_info.features[\"label\"].num_classes,\n",
    "                wd = wd)\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "  \n",
    "  log_dir = \"logs/tensorflow/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir,\n",
    "                                                        histogram_freq = 1)\n",
    "\n",
    "  lr_s = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "  # optimizer = tf.keras.optimizers.SGD(learning_rate = lr,\n",
    "  #                                     momentum = momentum)\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "  \n",
    "  model.compile(optimizer = optimizer,\n",
    "                loss = loss,\n",
    "                metrics = [\"accuracy\"])\n",
    "\n",
    "  history = model.fit(ds_train,\n",
    "                      epochs = epochs,\n",
    "                      validation_data = ds_test,\n",
    "                      callbacks = [lr_s,\n",
    "                                   tensorboard_callback])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6350b-101d-48b2-b37e-83caeec291e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom loop\n",
    "model = MgNet(iterations = iterations,\n",
    "              u_channels = u_channels,\n",
    "              f_channels = f_channels,\n",
    "              in_shape = ds_info.features[\"image\"].shape,\n",
    "              out_shape = ds_info.features[\"label\"].num_classes,\n",
    "              wd = wd)\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_loss = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "val_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "log_dir = \"logs/tensorflow/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_writer = tf.summary.create_file_writer(log_dir + \"/train\")\n",
    "val_writer = tf.summary.create_file_writer(log_dir + \"/validation\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  iterate = tqdm(enumerate(ds_train), total = \n",
    "                 -(ds_info.splits[\"train\"].num_examples // -batch_size))\n",
    "  for batch, (images, labels) in iterate:\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model(images, training = True)\n",
    "      loss_val = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_val, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    train_loss.update_state(loss_val)\n",
    "    train_acc.update_state(labels, logits)\n",
    "    iterate.set_description(f\"loss: {train_loss.result():.2f} \\\n",
    "                            - accuracy: {train_acc.result():.4f}\")\n",
    "    \n",
    "  for images, labels in ds_test:\n",
    "    logits = model(images, training = False)\n",
    "    val_loss.update_state(labels, logits)\n",
    "    val_acc.update_state(labels, logits)\n",
    "\n",
    "  log_weights(train_writer, model, epoch, grads)\n",
    "  with train_writer.as_default():\n",
    "    tf.summary.scalar(\"epoch_loss\", train_loss.result(), epoch)\n",
    "    tf.summary.scalar(\"epoch_accuracy\", train_acc.result(), epoch)\n",
    "  with val_writer.as_default():\n",
    "    tf.summary.scalar(\"epoch_loss\", val_loss.result(), epoch)\n",
    "    tf.summary.scalar(\"epoch_accuracy\", val_acc.result(), epoch)\n",
    "  \n",
    "  print(f\"epoch: {epoch + 1} - validation loss: {val_loss.result():.4f} - validation accuracy: {val_acc.result():.4f}\")\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_acc.reset_states()\n",
    "  val_loss.reset_states()\n",
    "  val_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5acf169f-5ecd-4a1c-a70f-2a69982dcaf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log weight initializations\n",
    "model = MgNet(iterations = iterations,\n",
    "              u_channels = u_channels,\n",
    "              f_channels = f_channels,\n",
    "              in_shape = ds_info.features[\"image\"].shape,\n",
    "              out_shape = ds_info.features[\"label\"].num_classes,\n",
    "              wd = wd)\n",
    "model.build((batch_size,) + ds_info.features[\"image\"].shape)\n",
    "writer = tf.summary.create_file_writer(\"logs/tensorflow/init\")\n",
    "log_weights(writer, model, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2eb8a6-cabf-479b-abb6-2772e22a05a5",
   "metadata": {},
   "source": [
    "# **PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e114a99f-b646-411d-848b-b8b7ba997149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb04870-f869-4906-9c48-5ec93d378f22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_data(path, minibatch_size, dataset):\n",
    "  if dataset == \"cifar100\":\n",
    "    normalize = torchvision.transforms.Normalize(mean=(0.5071, 0.4865, 0.4409),\n",
    "                                                 std=(0.2673, 0.2564, 0.2762))\n",
    "    transform_train = torchvision.transforms.Compose(\n",
    "      [torchvision.transforms.RandomCrop(32, padding = 4),\n",
    "       torchvision.transforms.RandomHorizontalFlip(),\n",
    "       torchvision.transforms.ToTensor(),\n",
    "       normalize])\n",
    "    transform_test  = torchvision.transforms.Compose(\n",
    "      [torchvision.transforms.ToTensor(),\n",
    "       normalize])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR100(root = path,\n",
    "                                             train = True,\n",
    "                                             download = True,\n",
    "                                             transform = transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                              batch_size = minibatch_size,\n",
    "                                              num_workers = 4,\n",
    "                                              shuffle = True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR100(root = path,\n",
    "                                            train = False,\n",
    "                                            download = True,\n",
    "                                            transform = transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset,\n",
    "                                             batch_size = minibatch_size,\n",
    "                                             num_workers = 4,\n",
    "                                             shuffle = False)\n",
    "    num_classes = 100\n",
    "  \n",
    "  return trainloader, testloader, num_classes\n",
    "\n",
    "trainloader, testloader, num_classes = load_data(\"~/pytorch_datasets\",\n",
    "                                                 128,\n",
    "                                                 \"cifar100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed7db32c-2601-49b8-90ee-7f04ab37aea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MgIte(nn.Module):\n",
    "  \n",
    "  def __init__(self,\n",
    "               A,\n",
    "               B):\n",
    "    super().__init__()\n",
    "    self.A = A\n",
    "    self.B = B        \n",
    "    self.bn1 = nn.BatchNorm2d(A.weight.size(0))\n",
    "    self.bn2 = nn.BatchNorm2d(B.weight.size(0))\n",
    "\n",
    "  def forward(self, out):\n",
    "    u, f = out\n",
    "    u = u + F.relu(self.bn2(self.B(F.relu(self.bn1((f - self.A(u)))))))\n",
    "    out = (u, f)\n",
    "    return out\n",
    "    \n",
    "class MgRestriction(nn.Module):\n",
    "  \n",
    "  def __init__(self,\n",
    "               A_old,\n",
    "               A_conv,\n",
    "               Pi_conv,\n",
    "               R_conv):\n",
    "    super().__init__()\n",
    "    self.A_old = A_old\n",
    "    self.A_conv = A_conv\n",
    "    self.Pi_conv = Pi_conv\n",
    "    self.R_conv = R_conv\n",
    "\n",
    "    self.bn1 = nn.BatchNorm2d(Pi_conv.weight.size(0))\n",
    "    self.bn2 = nn.BatchNorm2d(A_old.weight.size(0))\n",
    "\n",
    "  def forward(self, out):\n",
    "    u_old, f_old = out\n",
    "    u = F.relu(self.bn1(self.Pi_conv(u_old)))\n",
    "    f = F.relu(self.bn2(self.R_conv(f_old - self.A_old(u_old)))) + self.A_conv(u)\n",
    "    out = (u, f)\n",
    "    return out\n",
    "\n",
    "class MgNet(nn.Module):\n",
    "  \n",
    "  def __init__(self,\n",
    "               dataset,\n",
    "               num_iterations,\n",
    "               num_channel_f,\n",
    "               num_channel_u,\n",
    "               wise_B,\n",
    "               num_classes):\n",
    "    super().__init__()\n",
    "    self.num_iterations = num_iterations\n",
    "    self.num_channel_f = num_channel_f\n",
    "    self.num_channel_u = num_channel_u\n",
    "    self.wise_B = wise_B\n",
    "    \n",
    "    if dataset == \"mnist\":\n",
    "      self.num_channel_input = 1\n",
    "    else:\n",
    "      self.num_channel_input = 3\n",
    "      \n",
    "    self.conv1 = nn.Conv2d(self.num_channel_input,\n",
    "                           self.num_channel_f,\n",
    "                           kernel_size = 3,\n",
    "                           stride = 1,\n",
    "                           padding = 1,\n",
    "                           bias = False)\n",
    "    self.bn1 = nn.BatchNorm2d(self.num_channel_f)        \n",
    "\n",
    "    A_conv = nn.Conv2d(self.num_channel_u,\n",
    "                       self.num_channel_f,\n",
    "                       kernel_size = 3,\n",
    "                       stride = 1,\n",
    "                       padding = 1,\n",
    "                       bias = False)\n",
    "    if not self.wise_B:\n",
    "      B_conv = nn.Conv2d(self.num_channel_f,\n",
    "                         self.num_channel_u,\n",
    "                         kernel_size = 3,\n",
    "                         stride = 1,\n",
    "                         padding = 1,\n",
    "                         bias = False)\n",
    "    layers = []\n",
    "    for l, num_iteration_l in enumerate(self.num_iterations):\n",
    "      for i in range(num_iteration_l):\n",
    "        if self.wise_B:\n",
    "          B_conv = nn.Conv2d(self.num_channel_f,\n",
    "                             self.num_channel_u,\n",
    "                             kernel_size = 3,\n",
    "                             stride = 1,\n",
    "                             padding = 1,\n",
    "                             bias = False)\n",
    "        layers.append(MgIte(A_conv,\n",
    "                            B_conv))\n",
    "      setattr(self,\n",
    "              \"layer\" + str(l),\n",
    "              nn.Sequential(*layers))\n",
    "\n",
    "      if l < len(self.num_iterations) - 1:\n",
    "        A_old = A_conv\n",
    "        A_conv = nn.Conv2d(self.num_channel_u,\n",
    "                           self.num_channel_f,\n",
    "                           kernel_size = 3,\n",
    "                           stride = 1,\n",
    "                           padding = 1,\n",
    "                           bias = False)\n",
    "        if not self.wise_B:\n",
    "          B_conv = nn.Conv2d(self.num_channel_f,\n",
    "                             self.num_channel_u,\n",
    "                             kernel_size = 3,\n",
    "                             stride = 1,\n",
    "                             padding = 1,\n",
    "                             bias = False)\n",
    "        Pi_conv = nn.Conv2d(self.num_channel_u,\n",
    "                            self.num_channel_u,\n",
    "                            kernel_size = 3,\n",
    "                            stride = 2,\n",
    "                            padding = 1,\n",
    "                            bias = False)\n",
    "        R_conv = nn.Conv2d(self.num_channel_f,\n",
    "                           self.num_channel_u,\n",
    "                           kernel_size = 3,\n",
    "                           stride = 2,\n",
    "                           padding = 1,\n",
    "                           bias = False)\n",
    "        layers= [MgRestriction(A_old,\n",
    "                               A_conv,\n",
    "                               Pi_conv,\n",
    "                               R_conv)]\n",
    "\n",
    "    self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "    self.fc = nn.Linear(self.num_channel_u,\n",
    "                        num_classes)\n",
    "\n",
    "  def forward(self, u):\n",
    "    f = F.relu(self.bn1(self.conv1(u)))\n",
    "    if torch.cuda.is_available():\n",
    "      u = torch.zeros(f.size(),\n",
    "                      device = torch.device(\"cuda\"))\n",
    "    else:\n",
    "      u = torch.zeros(f.size())\n",
    "    out = (u, f)\n",
    "\n",
    "    for l in range(len(self.num_iterations)):\n",
    "      out = getattr(self,\n",
    "                    \"layer\" + str(l))(out)\n",
    "    u, f = out\n",
    "    u = self.pooling(u)\n",
    "    u = u.view(u.shape[0], -1)\n",
    "    u = self.fc(u)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86c295ca-c8e0-479d-82b0-d907db24dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no batchnorm\n",
    "\n",
    "class MgIte(nn.Module):\n",
    "  \n",
    "  def __init__(self,\n",
    "               A,\n",
    "               B):\n",
    "    super().__init__()\n",
    "    self.A = A\n",
    "    self.B = B        \n",
    "\n",
    "  def forward(self, out):\n",
    "    u, f = out\n",
    "    u = u + F.relu((self.B(F.relu(((f - self.A(u)))))))\n",
    "    out = (u, f)\n",
    "    return out\n",
    "    \n",
    "class MgRestriction(nn.Module):\n",
    "  \n",
    "  def __init__(self,\n",
    "               A_old,\n",
    "               A_conv,\n",
    "               Pi_conv,\n",
    "               R_conv):\n",
    "    super().__init__()\n",
    "    self.A_old = A_old\n",
    "    self.A_conv = A_conv\n",
    "    self.Pi_conv = Pi_conv\n",
    "    self.R_conv = R_conv\n",
    "\n",
    "  def forward(self, out):\n",
    "    u_old, f_old = out\n",
    "    u = F.relu((self.Pi_conv(u_old)))\n",
    "    f = F.relu((self.R_conv(f_old - self.A_old(u_old)))) + self.A_conv(u)\n",
    "    out = (u, f)\n",
    "    return out\n",
    "\n",
    "class MgNet(nn.Module):\n",
    "  \n",
    "  def __init__(self,\n",
    "               dataset,\n",
    "               num_iterations,\n",
    "               num_channel_f,\n",
    "               num_channel_u,\n",
    "               wise_B,\n",
    "               num_classes):\n",
    "    super().__init__()\n",
    "    self.num_iterations = num_iterations\n",
    "    self.num_channel_f = num_channel_f\n",
    "    self.num_channel_u = num_channel_u\n",
    "    self.wise_B = wise_B\n",
    "    \n",
    "    if dataset == \"mnist\":\n",
    "      self.num_channel_input = 1\n",
    "    else:\n",
    "      self.num_channel_input = 3\n",
    "      \n",
    "    self.conv1 = nn.Conv2d(self.num_channel_input,\n",
    "                           self.num_channel_f,\n",
    "                           kernel_size = 3,\n",
    "                           stride = 1,\n",
    "                           padding = 1,\n",
    "                           bias = False)\n",
    "\n",
    "    A_conv = nn.Conv2d(self.num_channel_u,\n",
    "                       self.num_channel_f,\n",
    "                       kernel_size = 3,\n",
    "                       stride = 1,\n",
    "                       padding = 1,\n",
    "                       bias = False)\n",
    "    if not self.wise_B:\n",
    "      B_conv = nn.Conv2d(self.num_channel_f,\n",
    "                         self.num_channel_u,\n",
    "                         kernel_size = 3,\n",
    "                         stride = 1,\n",
    "                         padding = 1,\n",
    "                         bias = False)\n",
    "    layers = []\n",
    "    for l, num_iteration_l in enumerate(self.num_iterations):\n",
    "      for i in range(num_iteration_l):\n",
    "        if self.wise_B:\n",
    "          B_conv = nn.Conv2d(self.num_channel_f,\n",
    "                             self.num_channel_u,\n",
    "                             kernel_size = 3,\n",
    "                             stride = 1,\n",
    "                             padding = 1,\n",
    "                             bias = False)\n",
    "        layers.append(MgIte(A_conv,\n",
    "                            B_conv))\n",
    "      setattr(self,\n",
    "              \"layer\" + str(l),\n",
    "              nn.Sequential(*layers))\n",
    "\n",
    "      if l < len(self.num_iterations) - 1:\n",
    "        A_old = A_conv\n",
    "        A_conv = nn.Conv2d(self.num_channel_u,\n",
    "                           self.num_channel_f,\n",
    "                           kernel_size = 3,\n",
    "                           stride = 1,\n",
    "                           padding = 1,\n",
    "                           bias = False)\n",
    "        if not self.wise_B:\n",
    "          B_conv = nn.Conv2d(self.num_channel_f,\n",
    "                             self.num_channel_u,\n",
    "                             kernel_size = 3,\n",
    "                             stride = 1,\n",
    "                             padding = 1,\n",
    "                             bias = False)\n",
    "        Pi_conv = nn.Conv2d(self.num_channel_u,\n",
    "                            self.num_channel_u,\n",
    "                            kernel_size = 3,\n",
    "                            stride = 2,\n",
    "                            padding = 1,\n",
    "                            bias = False)\n",
    "        R_conv = nn.Conv2d(self.num_channel_f,\n",
    "                           self.num_channel_u,\n",
    "                           kernel_size = 3,\n",
    "                           stride = 2,\n",
    "                           padding = 1,\n",
    "                           bias = False)\n",
    "        layers = [MgRestriction(A_old,\n",
    "                                A_conv,\n",
    "                                Pi_conv,\n",
    "                                R_conv)]\n",
    "\n",
    "    self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "    self.fc = nn.Linear(self.num_channel_u,\n",
    "                        num_classes)\n",
    "\n",
    "  def forward(self, u):\n",
    "    f = F.relu((self.conv1(u)))\n",
    "    if torch.cuda.is_available():\n",
    "      u = torch.zeros(f.size(),\n",
    "                      device = torch.device(\"cuda\"))\n",
    "    else:\n",
    "      u = torch.zeros(f.size())\n",
    "    out = (u, f)\n",
    "\n",
    "    for l in range(len(self.num_iterations)):\n",
    "      out = getattr(self,\n",
    "                    \"layer\" + str(l))(out)\n",
    "      u, f = out\n",
    "      # print(f[0][0][0])\n",
    "    u, f = out\n",
    "    u = self.pooling(u)\n",
    "    u = u.view(u.shape[0], -1)\n",
    "    u = self.fc(u)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f461ff-f398-4899-b731-d3ef381cd28d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_weights_pytorch(writer, model, epoch):\n",
    "  writer.add_histogram(\"mgnet_pytorch/initial_A_conv/kernel_0\", model.conv1.weight, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/initial_A_bn/gamma_0\", model.bn1.weight, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/initial_A_bn/beta_0\", model.bn1.bias, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/initial_A_bn/moving_mean_0\", model.bn1.running_mean, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/initial_A_bn/moving_variance_0\", model.bn1.running_var, epoch)\n",
    "  for l in range(len(model.num_iterations)):\n",
    "    seq = getattr(model, \"layer\" + str(l))\n",
    "    if l == 0:\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_conv/kernel_0\", seq[0].A.weight, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_conv/kernel_0\", seq[0].B.weight, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_batchnorm0/gamma_0\", seq[0].bn1.weight, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_batchnorm0/beta_0\", seq[0].bn1.bias, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_batchnorm0/moving_mean_0\", seq[0].bn1.running_mean, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_batchnorm0/moving_variance_0\", seq[0].bn1.running_var, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_batchnorm0/gamma_0\", seq[0].bn2.weight, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_batchnorm0/beta_0\", seq[0].bn2.bias, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_batchnorm0/moving_mean_0\", seq[0].bn2.running_mean, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_batchnorm0/moving_variance_0\", seq[0].bn2.running_var, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_batchnorm1/gamma_0\", seq[1].bn1.weight, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_batchnorm1/beta_0\", seq[1].bn1.bias, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_batchnorm1/moving_mean_0\", seq[1].bn1.running_mean, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_batchnorm1/moving_variance_0\", seq[1].bn1.running_var, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_batchnorm1/gamma_0\", seq[1].bn2.weight, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_batchnorm1/beta_0\", seq[1].bn2.bias, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_batchnorm1/moving_mean_0\", seq[1].bn2.running_mean, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_batchnorm1/moving_variance_0\", seq[1].bn2.running_var, epoch)\n",
    "    else:\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_Pi_conv/kernel_0\", seq[0].Pi_conv.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_R_conv/kernel_0\", seq[0].R_conv.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_Pi_batchnorm/gamma_0\", seq[0].bn1.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_Pi_batchnorm/beta_0\", seq[0].bn1.bias, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_Pi_batchnorm/moving_mean_0\", seq[0].bn1.running_mean, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_Pi_batchnorm/moving_variance_0\", seq[0].bn1.running_var, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_R_batchnorm/gamma_0\", seq[0].bn2.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_R_batchnorm/beta_0\", seq[0].bn2.bias, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_R_batchnorm/moving_mean_0\", seq[0].bn2.running_mean, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_R_batchnorm/moving_variance_0\", seq[0].bn2.running_var, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_conv/kernel_0\", seq[1].A.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_conv/kernel_0\", seq[1].B.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_batchnorm0/gamma_0\", seq[1].bn1.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_batchnorm0/beta_0\", seq[1].bn1.bias, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_batchnorm0/moving_mean_0\", seq[1].bn1.running_mean, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_batchnorm0/moving_variance_0\", seq[1].bn1.running_var, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_batchnorm0/gamma_0\", seq[1].bn2.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_batchnorm0/beta_0\", seq[1].bn2.bias, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_batchnorm0/moving_mean_0\", seq[1].bn2.running_mean, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_batchnorm0/moving_variance_0\", seq[1].bn2.running_var, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_batchnorm1/gamma_0\", seq[2].bn1.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_batchnorm1/beta_0\", seq[2].bn1.bias, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_batchnorm1/moving_mean_0\", seq[2].bn1.running_mean, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_batchnorm1/moving_variance_0\", seq[2].bn1.running_var, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_batchnorm1/gamma_0\", seq[2].bn2.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_batchnorm1/beta_0\", seq[2].bn2.bias, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_batchnorm1/moving_mean_0\", seq[2].bn2.running_mean, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_batchnorm1/moving_variance_0\", seq[2].bn2.running_var, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/output_softmax/kernel_0\", model.fc.weight, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/output_softmax/bias_0\", model.fc.bias, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e69979f-cab2-4ca9-ad5a-75e6b49de393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_weights_pytorch(writer, model, epoch):\n",
    "  # model = model.module\n",
    "  writer.add_histogram(\"mgnet_pytorch/initial_A_conv/kernel_0\", model.conv1.weight, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/initial_A_conv/kernel_0_grad\", model.conv1.weight.grad, epoch)\n",
    "  for l in range(len(model.num_iterations)):\n",
    "    seq = getattr(model, \"layer\" + str(l))\n",
    "    if l == 0:\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_conv/kernel_0\", seq[0].A.weight, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_conv/kernel_0\", seq[0].B.weight, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_A_conv/kernel_0_grad\", seq[0].A.weight.grad, epoch)\n",
    "      writer.add_histogram(\"mgnet_pytorch/block0/block0_B_conv/kernel_0_grad\", seq[0].B.weight.grad, epoch)\n",
    "    else:\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_Pi_conv/kernel_0\", seq[0].Pi_conv.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_R_conv/kernel_0\", seq[0].R_conv.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_conv/kernel_0\", seq[1].A.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_conv/kernel_0\", seq[1].B.weight, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_Pi_conv/kernel_0_grad\", seq[0].Pi_conv.weight.grad, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_R_conv/kernel_0_grad\", seq[0].R_conv.weight.grad, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_A_conv/kernel_0_grad\", seq[1].A.weight.grad, epoch)\n",
    "      writer.add_histogram(f\"mgnet_pytorch/block{l}/block{l}_B_conv/kernel_0_grad\", seq[1].B.weight.grad, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/output_softmax/kernel_0\", model.fc.weight, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/output_softmax/bias_0\", model.fc.bias, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/output_softmax/kernel_0_grad\", model.fc.weight.grad, epoch)\n",
    "  writer.add_histogram(\"mgnet_pytorch/output_softmax/bias_0_grad\", model.fc.bias.grad, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "39adf8f7-cfb3-4413-a787-ed85b1e35856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr):\n",
    "  if epoch == 0:\n",
    "    return init_lr\n",
    "  lr = init_lr * 0.1 ** (epoch // 30)\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group[\"lr\"] = lr\n",
    "  return lr\n",
    "\n",
    "def train_process(model, num_epochs, lr, trainloader, testloader):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  # optimizer = optim.SGD(model.parameters(),\n",
    "  #                       lr = lr,\n",
    "  #                       momentum = 0.9,\n",
    "  #                       weight_decay = 0.0005)\n",
    "  optimizer = optim.SGD(model.parameters(),\n",
    "                        lr = lr)\n",
    "\n",
    "  log_dir = \"logs/pytorch/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  train_writer = SummaryWriter(log_dir = log_dir + \"/train\")\n",
    "  val_writer = SummaryWriter(log_dir = log_dir + \"/validation\")\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    current_lr = adjust_learning_rate(optimizer, epoch, lr)\n",
    "    total_batches = -(50000 // -batch_size)\n",
    "    iterate = tqdm(enumerate(trainloader), total = \n",
    "                   total_batches)\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for i, (images, labels) in iterate:\n",
    "      if use_cuda:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "      outputs = model(images) \n",
    "      loss = criterion(outputs, labels)\n",
    "      total_train_loss += loss\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    total_val_batches = -(10000 // -batch_size)\n",
    "    def calculate_acc(loader, training, total_batches):\n",
    "      if training:\n",
    "        model.train()\n",
    "      if not training:\n",
    "        model.eval()\n",
    "      total_loss = 0\n",
    "      correct, total = 0, 0\n",
    "      for i, (images, labels) in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "          if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "          p_max, predicted = torch.max(outputs, 1) \n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum()\n",
    "          total_loss += loss\n",
    "      return float(correct) / total, total_loss / total_batches\n",
    "\n",
    "    train_acc, train_loss = calculate_acc(trainloader, True, total_batches)\n",
    "    val_acc, val_loss = calculate_acc(testloader, False, total_val_batches)\n",
    "    \n",
    "    train_writer.add_scalar(\"epoch_loss\", train_loss, epoch)\n",
    "    train_writer.add_scalar(\"epoch_accuracy\", train_acc, epoch)\n",
    "    val_writer.add_scalar(\"epoch_loss\", val_loss, epoch)\n",
    "    val_writer.add_scalar(\"epoch_accuracy\", val_acc, epoch)\n",
    "    \n",
    "    log_weights_pytorch(train_writer, model, epoch)\n",
    "    \n",
    "    print(f\"training loss: {train_loss} - validation loss: {val_loss}\")\n",
    "    print(f\"epoch: {epoch + 1} - training accuracy: {train_acc} - validation accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1f67a-0112-482c-a54c-92d54681c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MgNet(dataset = dataset,\n",
    "              num_iterations = iterations,\n",
    "              num_channel_f = 256,\n",
    "              num_channel_u = 256,\n",
    "              wise_B = False,\n",
    "              num_classes = 100)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "train_process(model = model,\n",
    "              num_epochs = 150,\n",
    "              lr = .1,\n",
    "              trainloader = trainloader,\n",
    "              testloader = testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "521de6ab-b866-4de7-ade9-b3a0638299aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log weight initializations\n",
    "model = MgNet(dataset = dataset,\n",
    "              num_iterations = iterations,\n",
    "              num_channel_f = 256,\n",
    "              num_channel_u = 256,\n",
    "              wise_B = False,\n",
    "              num_classes = 100)\n",
    "model = model.cuda()\n",
    "writer = SummaryWriter(log_dir = \"logs/pytorch/init\")\n",
    "init_iter = iter(trainloader)\n",
    "images, labels = init_iter.next()\n",
    "writer.add_graph(model, images.cuda())\n",
    "log_weights_pytorch(writer, model, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b44d0-3320-4d1c-b1f7-c52b88d84a3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2dfe583-29ab-4405-81c7-0fa0f22a29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for batch, (images, labels) in enumerate(trainloader):\n",
    "  dataset.append((images, labels))\n",
    "test_dataset = []\n",
    "for batch, (images, labels) in enumerate(testloader):\n",
    "  test_dataset.append((images, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f9c067-1dcc-4503-b9cc-f15a3c4bde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bound(shape):  \n",
    "  gain = np.sqrt(2.0 / (1 + np.sqrt(5) ** 2))\n",
    "  torch_shape = np.flip(shape) \n",
    "  num_input_fmaps = torch_shape[1]\n",
    "  receptive_field_size = 1\n",
    "  if len(torch_shape) > 2:\n",
    "    for s in torch_shape[2:]:\n",
    "      receptive_field_size *= s\n",
    "  fan_in = num_input_fmaps * receptive_field_size\n",
    "\n",
    "  std = gain / np.sqrt(fan_in)\n",
    "  return np.sqrt(3.0) * std\n",
    "\n",
    "A_bound = calc_bound((3, 3, 3, 256))\n",
    "A_init = np.random.uniform(low = -A_bound,\n",
    "                           high = A_bound,\n",
    "                           size = (3, 3, 3, 256))\n",
    "conv_bound = calc_bound((3, 3, 256, 256))\n",
    "conv_init = []\n",
    "for i in range(14):\n",
    "  conv_init.append(np.random.uniform(low = -conv_bound,\n",
    "                                     high = conv_bound,\n",
    "                                     size = (3, 3, 256, 256)))\n",
    "fc_bound = calc_bound((256, 100))\n",
    "fc_init = np.random.uniform(low = -fc_bound,\n",
    "                            high = fc_bound,\n",
    "                            size = (256, 100))\n",
    "bias_bound = 1 / np.sqrt(256)\n",
    "bias_init = np.random.uniform(low = -bias_bound,\n",
    "                              high = bias_bound,\n",
    "                              size = (100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957a4dbe-cdb5-477c-ab58-aaaac7b14f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorflow test\n",
    "\n",
    "model_tf = MgNet(iterations = iterations,\n",
    "              u_channels = u_channels,\n",
    "              f_channels = f_channels,\n",
    "              in_shape = ds_info.features[\"image\"].shape,\n",
    "              out_shape = ds_info.features[\"label\"].num_classes,\n",
    "              wd = wd)\n",
    "model_tf.build((128, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf4ed101-4627-4072-ac92-ef3ac71c9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf.A_init.weights[0] = tf.Variable(initial_value = A_init)\n",
    "model_tf.blocks[0].A.weights[0] = tf.Variable(initial_value = conv_init[0])\n",
    "model_tf.blocks[0].B.weights[0] = tf.Variable(initial_value = conv_init[1])\n",
    "model_tf.blocks[1].Pi.weights[0] = tf.Variable(initial_value = conv_init[2])\n",
    "model_tf.blocks[1].R.weights[0] = tf.Variable(initial_value = conv_init[3])\n",
    "model_tf.blocks[1].MgSmooth.A.weights[0] = tf.Variable(initial_value = conv_init[4])\n",
    "model_tf.blocks[1].MgSmooth.B.weights[0] = tf.Variable(initial_value = conv_init[5])\n",
    "model_tf.blocks[2].Pi.weights[0] = tf.Variable(initial_value = conv_init[6])\n",
    "model_tf.blocks[2].R.weights[0] = tf.Variable(initial_value = conv_init[7])\n",
    "model_tf.blocks[2].MgSmooth.A.weights[0] = tf.Variable(initial_value = conv_init[8])\n",
    "model_tf.blocks[2].MgSmooth.B.weights[0] = tf.Variable(initial_value = conv_init[9])\n",
    "model_tf.blocks[3].Pi.weights[0] = tf.Variable(initial_value = conv_init[10])\n",
    "model_tf.blocks[3].R.weights[0] = tf.Variable(initial_value = conv_init[11])\n",
    "model_tf.blocks[3].MgSmooth.A.weights[0] = tf.Variable(initial_value = conv_init[12])\n",
    "model_tf.blocks[3].MgSmooth.B.weights[0] = tf.Variable(initial_value = conv_init[13])\n",
    "model_tf.fc.weights[0] = tf.Variable(initial_value = fc_init)\n",
    "model_tf.fc.weights[1] = tf.Variable(initial_value = bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ecd3f1f-5bff-4276-87ac-c12713dc5ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf(tf.transpose(tf.convert_to_tensor(dataset[0][0].numpy()), [0, 2, 3, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9223df-6359-4017-9643-60d49536dc08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = lr)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_loss = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "val_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "log_dir = \"logs/tensorflow/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_writer = tf.summary.create_file_writer(log_dir + \"/train\")\n",
    "val_writer = tf.summary.create_file_writer(log_dir + \"/validation\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  iterate = tqdm(enumerate(dataset), total = \n",
    "                 -(ds_info.splits[\"train\"].num_examples // -batch_size))\n",
    "  for batch, (images, labels) in iterate:\n",
    "    images = tf.transpose(tf.convert_to_tensor(images.numpy()), [0, 2, 3, 1])\n",
    "    labels = tf.convert_to_tensor(labels.numpy())\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model_tf(images, training = True)\n",
    "      loss_val = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_val, model_tf.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model_tf.trainable_weights))\n",
    "    \n",
    "    train_loss.update_state(loss_val)\n",
    "    train_acc.update_state(labels, logits)\n",
    "    iterate.set_description(f\"loss: {train_loss.result():.2f} \\\n",
    "                            - accuracy: {train_acc.result():.4f}\")\n",
    "    \n",
    "  for images, labels in test_dataset:\n",
    "    images = tf.transpose(tf.convert_to_tensor(images.numpy()), [0, 2, 3, 1])\n",
    "    labels = tf.convert_to_tensor(labels.numpy())\n",
    "    logits = model_tf(images, training = False)\n",
    "    val_loss.update_state(labels, logits)\n",
    "    val_acc.update_state(labels, logits)\n",
    "\n",
    "  log_weights(train_writer, model_tf, epoch, grads)\n",
    "  with train_writer.as_default():\n",
    "    tf.summary.scalar(\"epoch_loss\", train_loss.result(), epoch)\n",
    "    tf.summary.scalar(\"epoch_accuracy\", train_acc.result(), epoch)\n",
    "  with val_writer.as_default():\n",
    "    tf.summary.scalar(\"epoch_loss\", val_loss.result(), epoch)\n",
    "    tf.summary.scalar(\"epoch_accuracy\", val_acc.result(), epoch)\n",
    "  \n",
    "  print(f\"epoch: {epoch + 1} - validation loss: {val_loss.result():.4f} - validation accuracy: {val_acc.result():.4f}\")\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_acc.reset_states()\n",
    "  val_loss.reset_states()\n",
    "  val_acc.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e566df4a-874c-4ad7-92a2-8a3ff08243dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch test\n",
    "\n",
    "model_pt = MgNet(dataset = dataset,\n",
    "              num_iterations = iterations,\n",
    "              num_channel_f = 256,\n",
    "              num_channel_u = 256,\n",
    "              wise_B = False,\n",
    "              num_classes = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd7224bf-b25c-4b67-9c30-6edfea06b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt.conv1.weight = nn.Parameter(torch.from_numpy(np.transpose(A_init, [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer0\")[0].A.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[0], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer0\")[0].B.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[1], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer1\")[0].Pi_conv.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[2], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer1\")[0].R_conv.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[3], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer1\")[1].A.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[4], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer1\")[1].B.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[5], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer2\")[0].Pi_conv.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[6], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer2\")[0].R_conv.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[7], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer2\")[1].A.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[8], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer2\")[1].B.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[9], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer3\")[0].Pi_conv.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[10], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer3\")[0].R_conv.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[11], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer3\")[1].A.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[12], [3, 2, 0, 1])).float())\n",
    "getattr(model_pt, \"layer3\")[1].B.weight = nn.Parameter(torch.from_numpy(np.transpose(conv_init[13], [3, 2, 0, 1])).float())\n",
    "model_pt.fc.weight = nn.Parameter(torch.from_numpy(np.transpose(fc_init, [1, 0])).float())\n",
    "model_pt.fc.bias = nn.Parameter(torch.from_numpy(bias_init).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb97a217-fddd-4f8f-b379-60025c0bf80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.3921e-03, -3.7010e-02,  1.9178e-02,  ...,  1.1835e-02,\n",
       "         -8.6755e-03, -6.8535e-02],\n",
       "        [ 8.7179e-03, -3.3621e-02,  2.6003e-02,  ...,  1.2554e-02,\n",
       "         -7.5911e-03, -6.9567e-02],\n",
       "        [ 2.8036e-02, -2.4973e-02,  3.3461e-02,  ...,  1.4026e-02,\n",
       "         -3.5621e-03, -7.0676e-02],\n",
       "        ...,\n",
       "        [ 4.5102e-03, -4.1469e-02,  1.7230e-02,  ...,  1.0431e-02,\n",
       "         -3.6491e-03, -6.3158e-02],\n",
       "        [ 2.3616e-03, -4.7221e-02,  9.6849e-03,  ...,  1.0257e-02,\n",
       "         -5.8721e-03, -6.5567e-02],\n",
       "        [ 1.8216e-02, -3.5550e-02,  2.4981e-02,  ...,  1.0037e-02,\n",
       "         -2.5053e-06, -6.2280e-02]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = dataset[0][0].cuda()\n",
    "model_pt = model_pt.cuda()\n",
    "model_pt(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5e1af9c7-519b-4ce8-90ee-c444c05efffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr):\n",
    "  if epoch == 0:\n",
    "    return init_lr\n",
    "  lr = init_lr * 0.1 ** (epoch // 30)\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group[\"lr\"] = lr\n",
    "  return lr\n",
    "\n",
    "def train_process(model, num_epochs, lr, trainloader, testloader):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  # optimizer = optim.SGD(model.parameters(),\n",
    "  #                       lr = lr,\n",
    "  #                       momentum = 0.9,\n",
    "  #                       weight_decay = 0.0005)\n",
    "  optimizer = optim.SGD(model.parameters(),\n",
    "                        lr = lr)\n",
    "\n",
    "  log_dir = \"logs/pytorch/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  train_writer = SummaryWriter(log_dir = log_dir + \"/train\")\n",
    "  val_writer = SummaryWriter(log_dir = log_dir + \"/validation\")\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    current_lr = adjust_learning_rate(optimizer, epoch, lr)\n",
    "    total_batches = -(50000 // -batch_size)\n",
    "    iterate = tqdm(enumerate(trainloader), total = \n",
    "                   total_batches)\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for i, (images, labels) in iterate:\n",
    "      if use_cuda:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "      outputs = model(images) \n",
    "      loss = criterion(outputs, labels)\n",
    "      total_train_loss += loss\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    total_val_batches = -(10000 // -batch_size)\n",
    "    def calculate_acc(loader, training, total_batches):\n",
    "      if training:\n",
    "        model.train()\n",
    "      if not training:\n",
    "        model.eval()\n",
    "      total_loss = 0\n",
    "      correct, total = 0, 0\n",
    "      for i, (images, labels) in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "          if use_cuda:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "          outputs = model(images)\n",
    "          loss = criterion(outputs, labels)\n",
    "          p_max, predicted = torch.max(outputs, 1) \n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum()\n",
    "          total_loss += loss\n",
    "      return float(correct) / total, total_loss / total_batches\n",
    "\n",
    "    train_acc, train_loss = calculate_acc(trainloader, True, total_batches)\n",
    "    val_acc, val_loss = calculate_acc(testloader, False, total_val_batches)\n",
    "    \n",
    "    train_writer.add_scalar(\"epoch_loss\", train_loss, epoch)\n",
    "    train_writer.add_scalar(\"epoch_accuracy\", train_acc, epoch)\n",
    "    val_writer.add_scalar(\"epoch_loss\", val_loss, epoch)\n",
    "    val_writer.add_scalar(\"epoch_accuracy\", val_acc, epoch)\n",
    "    \n",
    "    log_weights_pytorch(train_writer, model, epoch)\n",
    "    \n",
    "    print(f\"training loss: {train_loss} - validation loss: {val_loss}\")\n",
    "    print(f\"epoch: {epoch + 1} - training accuracy: {train_acc} - validation accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e19ba-8458-4130-aa69-ccdfd6a9f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt = model_pt.cuda()\n",
    "train_process(model = model_pt,\n",
    "              num_epochs = 150,\n",
    "              lr = .1,\n",
    "              trainloader = dataset,\n",
    "              testloader = test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ed5a6b0-88aa-425a-8e09-6a1d80e0cb03",
   "metadata": {},
   "source": [
    "# **MgNet SCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModule(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self,\n",
    "                channels,\n",
    "                wd):\n",
    "    super(AttentionModule, self).__init__()\n",
    "\n",
    "    self.conv = Conv2D(channels,\n",
    "                        (3, 3),\n",
    "                        strides = (2, 2),\n",
    "                        padding = (1, 1),\n",
    "                        use_bias = False,\n",
    "                        kernel_regularizer = \n",
    "                          tf.keras.regularizers.L2(wd))\n",
    "    self.bn1 = tf.keras.layers.BatchNormalization(momentum = .9,\n",
    "                                                  epsilon = 1e-5)\n",
    "    self.deconv = tf.keras.layers.Conv2DTranspose(channels,\n",
    "                                                  (3, 3),\n",
    "                                                  strides = (2, 2),\n",
    "                                                  padding = (1, 1),\n",
    "                                                  use_bias = False,\n",
    "                                                  kernel_regularizer = \n",
    "                                                    tf.keras.regularizers.L2(wd))\n",
    "    self.bn2 = tf.keras.layers.BatchNormalization(momentum = .9,\n",
    "                                                  epsilon = 1e-5)\n",
    "\n",
    "    self.attn_conv1 = tf.keras.layers.Conv2D()\n",
    "    self.attn_conv2 = tf.keras.layers.Conv2D()\n",
    "\n",
    "  def call(self, u):\n",
    "    u_d = self.conv(u)\n",
    "    u_d = tf.nn.relu(self.bn1(u_d))\n",
    "    u_d = self.deconv(u_d)\n",
    "    u_d = tf.math.sigmoid(self.bn2(u_d))\n",
    "\n",
    "    u_a = self.attn_conv1(u)\n",
    "    u_a = self.attn_conv2(u_a)\n",
    "\n",
    "    u = tf.matmul(u_d, u_a)\n",
    "    return u\n",
    "\n",
    "class ShallowClassifier(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self,\n",
    "               in_shape,\n",
    "               out_shape):\n",
    "    super(ShallowClassifier, self).__init__()\n",
    "\n",
    "    self.bottleneck_channels = 512\n",
    "    self.bottleneck = tf.keras.layers.Dense(self.bottleneck_channels,\n",
    "                                            kernel_initializer = \n",
    "                                              HeUniform(np.sqrt(5),\n",
    "                                                        \"fan_in\",\n",
    "                                                        \"leaky_relu\"),\n",
    "                                            bias_initializer = \n",
    "                                              HeUniform(np.sqrt(5),\n",
    "                                                        \"fan_in\",\n",
    "                                                        \"leaky_relu\",\n",
    "                                                        1 / np.sqrt(in_shape[-1])),\n",
    "                                            kernel_regularizer = \n",
    "                                              tf.keras.regularizers.L2(wd))\n",
    "    self.fc = tf.keras.layers.Dense(out_shape,\n",
    "                                    kernel_initializer = \n",
    "                                      HeUniform(np.sqrt(5),\n",
    "                                                \"fan_in\",\n",
    "                                                \"leaky_relu\"),\n",
    "                                    bias_initializer = \n",
    "                                      HeUniform(np.sqrt(5),\n",
    "                                                \"fan_in\",\n",
    "                                                \"leaky_relu\",\n",
    "                                                1 / np.sqrt(self.bottleneck_channels)),\n",
    "                                    kernel_regularizer = \n",
    "                                      tf.keras.regularizers.L2(wd))\n",
    "\n",
    "  def call(self, u):\n",
    "    u = self.bottleneck(u)\n",
    "    u = self.fc(u)\n",
    "    return u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
