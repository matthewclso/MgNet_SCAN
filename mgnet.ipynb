{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1779d667-58c1-4758-9593-e7372d7094dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cifar10\"\n",
    "iterations = \"2,2,2,2\"\n",
    "u_channels = \"256,256,256,256\"\n",
    "f_channels = \"256,256,256,256\"\n",
    "batch_size = 1024\n",
    "epochs = 100\n",
    "lr = .01\n",
    "graph = True\n",
    "\n",
    "iterations = [int(x) for x in iterations.split(\",\")]\n",
    "u_channels = [int(x) for x in u_channels.split(\",\")]\n",
    "f_channels = [int(x) for x in f_channels.split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "110092fb-21d8-43ad-9647-23c553c84e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae112eb-50d0-4b98-90d1-2f768d69307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    dataset,\n",
    "    split = [\"train\", \"test\"],\n",
    "    shuffle_files = True,\n",
    "    as_supervised = True,\n",
    "    with_info = True\n",
    ")\n",
    "\n",
    "def preprocess(ds):\n",
    "  def normalize_img(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "  ds = ds.map(normalize_img, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "  ds = ds.cache()\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  return ds\n",
    "\n",
    "ds_train = preprocess(ds_train)\n",
    "ds_test = preprocess(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00340774-1918-40b6-bdc7-4a5d22b1c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MgSmooth(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, iterations, u_channels, f_channels):\n",
    "    super(MgSmooth, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.A = tf.keras.layers.Conv2D(u_channels, (3, 3), strides = (1, 1), padding = \"same\", use_bias = False)\n",
    "    self.B = tf.keras.layers.Conv2D(f_channels, (3, 3), strides = (1, 1), padding = \"same\", use_bias = False)\n",
    "\n",
    "    self.A_bns, self.B_bns = [], []\n",
    "    for _ in range(self.iterations):\n",
    "      self.A_bns.append(tf.keras.layers.BatchNormalization(axis = 1))\n",
    "      self.B_bns.append(tf.keras.layers.BatchNormalization(axis = 1))\n",
    "\n",
    "  def call(self, u, f):\n",
    "    for i in range(self.iterations):\n",
    "      error = tf.nn.relu(self.A_bns[i](f - self.A(u)))\n",
    "      u = u + tf.nn.relu(self.B_bns[i](self.B(error)))\n",
    "    return u, f\n",
    "\n",
    "class MgBlock(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, iterations, u_channels, f_channels, A_old):\n",
    "    super(MgBlock, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.Pi = tf.keras.layers.Conv2D(u_channels, (3, 3), strides = (2, 2), padding = \"same\", use_bias = False)\n",
    "    self.R = tf.keras.layers.Conv2D(f_channels, (3, 3), strides = (2, 2), padding = \"same\", use_bias = False)\n",
    "    self.A_old = A_old\n",
    "    self.MgSmooth = MgSmooth(self.iterations, u_channels, f_channels)\n",
    "\n",
    "    self.Pi_bn = tf.keras.layers.BatchNormalization(axis = 1)\n",
    "    self.R_bn = tf.keras.layers.BatchNormalization(axis = 1)\n",
    "\n",
    "  def call(self, u0, f0):\n",
    "    u1 = tf.nn.relu(self.Pi_bn(self.Pi(u0)))\n",
    "    error = tf.nn.relu(self.R_bn(self.Pi(f0 - self.A_old(u0))))\n",
    "    f1 = error + self.MgSmooth.A(u1)\n",
    "    u, f = self.MgSmooth(u1, f1)\n",
    "    return u, f\n",
    "\n",
    "class MgNet(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, iterations, u_channels, f_channels, in_shape, out_shape):\n",
    "    super(MgNet, self).__init__()\n",
    "\n",
    "    self.iterations = iterations\n",
    "    self.in_shape = in_shape\n",
    "    self.A_init = tf.keras.layers.Conv2D(u_channels[0], (3, 3), strides = (1, 1), padding = \"same\", use_bias = False)\n",
    "    self.A_bn = tf.keras.layers.BatchNormalization(axis = 1)\n",
    "\n",
    "    self.A0 = tf.keras.layers.Conv2D(u_channels[0], (3, 3), strides = (1, 1), padding = \"same\", use_bias = False)\n",
    "    self.blocks = []\n",
    "    for i in range(len(self.iterations)):\n",
    "      if i == 0:\n",
    "        self.blocks.append(MgSmooth(iterations[i], u_channels[i], f_channels[i]))\n",
    "        continue\n",
    "      if i == 1:\n",
    "        self.blocks.append(MgBlock(iterations[i], u_channels[i], f_channels[i], self.A0))\n",
    "        continue\n",
    "      self.blocks.append(MgBlock(iterations[i], u_channels[i], f_channels[i], self.blocks[i - 1].MgSmooth.A))\n",
    "\n",
    "    x = in_shape[0]\n",
    "    for i in range(len(self.blocks) - 1):\n",
    "      x = ((x + 2 - 3) // 2) + 1\n",
    "    self.pool = tf.keras.layers.AveragePooling2D(pool_size = (x, x))\n",
    "    self.softmax = tf.keras.layers.Dense(out_shape, activation = \"softmax\")\n",
    "  \n",
    "  def call(self, u0):\n",
    "    f = tf.nn.relu(self.A_bn(self.A_init(u0)))\n",
    "    u = tf.multiply(f, 0)\n",
    "\n",
    "    for block in self.blocks:\n",
    "      u, f = block(u, f)\n",
    "    u = self.pool(u)\n",
    "    u = tf.squeeze(u, [-2, -3])\n",
    "    u = self.softmax(u)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e45c82-9eae-42dd-9dab-f291eb6047fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 59s 200ms/step - loss: 2.6456 - accuracy: 0.1172 - val_loss: 24.0521 - val_accuracy: 0.1423\n",
      "Epoch 2/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 2.2530 - accuracy: 0.1499 - val_loss: 4.0243 - val_accuracy: 0.1652\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 2.0856 - accuracy: 0.2218 - val_loss: 2.4289 - val_accuracy: 0.2220\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 1.9685 - accuracy: 0.2744 - val_loss: 1.9371 - val_accuracy: 0.2885\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 1.8284 - accuracy: 0.3200 - val_loss: 1.9097 - val_accuracy: 0.2865\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 1.7349 - accuracy: 0.3601 - val_loss: 1.8157 - val_accuracy: 0.3214\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 1.6604 - accuracy: 0.3867 - val_loss: 1.8408 - val_accuracy: 0.3138\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 1.5865 - accuracy: 0.4153 - val_loss: 1.8279 - val_accuracy: 0.3186\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - 14s 138ms/step - loss: 1.5159 - accuracy: 0.4458 - val_loss: 1.6479 - val_accuracy: 0.3825\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 1.4467 - accuracy: 0.4745 - val_loss: 1.5154 - val_accuracy: 0.4409\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - 15s 155ms/step - loss: 1.3860 - accuracy: 0.4976 - val_loss: 1.4535 - val_accuracy: 0.4753\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 1.3172 - accuracy: 0.5245 - val_loss: 1.3644 - val_accuracy: 0.5016\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 1.2453 - accuracy: 0.5511 - val_loss: 1.2676 - val_accuracy: 0.5422\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - 14s 141ms/step - loss: 1.1754 - accuracy: 0.5807 - val_loss: 1.2095 - val_accuracy: 0.5674\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 1.1178 - accuracy: 0.6011 - val_loss: 1.1617 - val_accuracy: 0.5849\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - 13s 138ms/step - loss: 1.0485 - accuracy: 0.6272 - val_loss: 1.1305 - val_accuracy: 0.5934\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 0.9767 - accuracy: 0.6535 - val_loss: 1.1790 - val_accuracy: 0.5823\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - 13s 137ms/step - loss: 0.9140 - accuracy: 0.6747 - val_loss: 1.1871 - val_accuracy: 0.5789\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - 14s 142ms/step - loss: 0.8582 - accuracy: 0.6941 - val_loss: 1.0828 - val_accuracy: 0.6178\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - 13s 137ms/step - loss: 0.8131 - accuracy: 0.7118 - val_loss: 1.0769 - val_accuracy: 0.6269\n",
      "Epoch 21/100\n",
      "98/98 [==============================] - 14s 138ms/step - loss: 0.7691 - accuracy: 0.7289 - val_loss: 1.2025 - val_accuracy: 0.5922\n",
      "Epoch 22/100\n",
      "98/98 [==============================] - 13s 137ms/step - loss: 0.7120 - accuracy: 0.7488 - val_loss: 1.2254 - val_accuracy: 0.5949\n",
      "Epoch 23/100\n",
      "98/98 [==============================] - 14s 141ms/step - loss: 0.6595 - accuracy: 0.7681 - val_loss: 1.2106 - val_accuracy: 0.6010\n",
      "Epoch 24/100\n",
      "98/98 [==============================] - 14s 142ms/step - loss: 0.5853 - accuracy: 0.7951 - val_loss: 1.4499 - val_accuracy: 0.5585\n",
      "Epoch 25/100\n",
      "98/98 [==============================] - 14s 138ms/step - loss: 0.5579 - accuracy: 0.8028 - val_loss: 1.4901 - val_accuracy: 0.5682\n",
      "Epoch 26/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 0.5421 - accuracy: 0.8062 - val_loss: 1.2475 - val_accuracy: 0.6132\n",
      "Epoch 27/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 0.4615 - accuracy: 0.8358 - val_loss: 1.3775 - val_accuracy: 0.5888\n",
      "Epoch 28/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 0.4330 - accuracy: 0.8445 - val_loss: 1.2699 - val_accuracy: 0.6315\n",
      "Epoch 29/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 0.3743 - accuracy: 0.8694 - val_loss: 1.3593 - val_accuracy: 0.6251\n",
      "Epoch 30/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 0.3197 - accuracy: 0.8863 - val_loss: 1.4739 - val_accuracy: 0.6113\n",
      "Epoch 31/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 0.3143 - accuracy: 0.8873 - val_loss: 1.4417 - val_accuracy: 0.6150\n",
      "Epoch 32/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 0.2603 - accuracy: 0.9084 - val_loss: 1.6365 - val_accuracy: 0.6056\n",
      "Epoch 33/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 0.2568 - accuracy: 0.9102 - val_loss: 1.5303 - val_accuracy: 0.6115\n",
      "Epoch 34/100\n",
      "98/98 [==============================] - 14s 141ms/step - loss: 0.2492 - accuracy: 0.9110 - val_loss: 1.5260 - val_accuracy: 0.6062\n",
      "Epoch 35/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 0.2171 - accuracy: 0.9229 - val_loss: 1.5091 - val_accuracy: 0.6309\n",
      "Epoch 36/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 0.1815 - accuracy: 0.9367 - val_loss: 1.5549 - val_accuracy: 0.6371\n",
      "Epoch 37/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 0.1416 - accuracy: 0.9520 - val_loss: 1.6450 - val_accuracy: 0.6361\n",
      "Epoch 38/100\n",
      "98/98 [==============================] - 14s 140ms/step - loss: 0.1258 - accuracy: 0.9583 - val_loss: 1.8786 - val_accuracy: 0.6268\n",
      "Epoch 39/100\n",
      "98/98 [==============================] - 14s 139ms/step - loss: 0.1191 - accuracy: 0.9617 - val_loss: 1.8356 - val_accuracy: 0.6337\n",
      "Epoch 40/100\n",
      "98/98 [==============================] - 14s 141ms/step - loss: 0.0917 - accuracy: 0.9708 - val_loss: 1.7094 - val_accuracy: 0.6429\n",
      "Epoch 41/100\n",
      "98/98 [==============================] - 14s 142ms/step - loss: 0.0795 - accuracy: 0.9752 - val_loss: 1.7468 - val_accuracy: 0.6493\n",
      "Epoch 42/100\n",
      "98/98 [==============================] - 16s 159ms/step - loss: 0.0687 - accuracy: 0.9788 - val_loss: 1.8283 - val_accuracy: 0.6526\n",
      "Epoch 43/100\n",
      "98/98 [==============================] - 14s 142ms/step - loss: 0.0624 - accuracy: 0.9807 - val_loss: 1.9188 - val_accuracy: 0.6246\n",
      "Epoch 44/100\n",
      "73/98 [=====================>........] - ETA: 3s - loss: 0.0681 - accuracy: 0.9786"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "with strategy.scope():\n",
    "  model = MgNet(iterations,\n",
    "                u_channels,\n",
    "                f_channels,\n",
    "                ds_info.features[\"image\"].shape,\n",
    "                ds_info.features[\"label\"].num_classes)\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "  optimizer = tf.keras.optimizers.Adam(lr)\n",
    "  model.compile(optimizer = optimizer, loss = loss, metrics = [\"accuracy\"])\n",
    "\n",
    "  history = model.fit(ds_train,\n",
    "                      epochs = epochs,\n",
    "                      validation_data = ds_test)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609c21a-9301-487a-ab14-4bb270321d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if graph:\n",
    "  loss = history.history[\"loss\"]\n",
    "  accuracy = history.history[\"accuracy\"]\n",
    "  val_loss = history.history[\"val_loss\"]\n",
    "  val_accuracy = history.history[\"val_accuracy\"]\n",
    "  timerange = range(len(loss))\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  train_loss_plot, = ax.plot(timerange, loss, color = \"blue\")\n",
    "  val_loss_plot, = ax.plot(timerange, val_loss, color = \"cyan\")\n",
    "  train_loss_plot.set_label(\"Train Loss\")\n",
    "  val_loss_plot.set_label(\"Validation Loss\")\n",
    "  ax.set_xlabel(\"Epoch\")\n",
    "  ax.set_ylabel(\"Loss\")\n",
    "  ax.legend(loc = \"upper left\")\n",
    "  ax2 = ax.twinx()\n",
    "  train_acc_plot, = ax2.plot(timerange, accuracy, color = \"purple\")\n",
    "  val_acc_plot, = ax2.plot(timerange, val_accuracy, color = \"pink\")\n",
    "  train_acc_plot.set_label(\"Train Accuracy\")\n",
    "  val_acc_plot.set_label(\"Validation Accuracy\")\n",
    "  ax2.set_ylabel(\"Accuracy\")\n",
    "  ax2.legend(loc = \"upper right\")\n",
    "  plt.title(\"Loss vs Accuracy\")\n",
    "  plt.savefig(f\"{dataset}_mgnet_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
